\chapter{Service Function Chaining Implementation}
\label{chap:impl}

\newcommand{\enchainer}{\texttt{Enchainer}}
\newcommand{\vnf}{\texttt{VNF}}
\newcommand{\vnfs}{\texttt{VNFs}}
\newcommand{\dispatcher}{\texttt{Dispatcher}}
\newcommand{\astaire}{\texttt{Astaire}}
\newcommand{\ironhide}{\texttt{Ironhide}}
\newcommand{\harbor}{\texttt{Harbor}}
\newcommand{\roulette}{\texttt{Roulette}}
\newcommand{\ingress}{\texttt{ingress}}
\newcommand{\ingresses}{\texttt{ingresses}}
\newcommand{\egress}{\texttt{egress}}
\newcommand{\egresses}{\texttt{egresses}}

In this chapter is described the implementation of the solution that we
proposed for the SFC system. Several design choices and iterations are
presented. Moreover, along with the final system description we detail the
experimental testbed and the carried tests. 

\section{Kubernetes Pods and our implementation}
All components of the SFC that are presented and we developed are thought to
run inside Docker containers. This approach allowed us to be independent on the
platform on which they will be deployed and to execute them on
Kubernetes\footnote{\url{https://kubernetes.io/docs/concepts/workloads/pods/pod/}}.
Pods are group of one or more containers that share storage and network
resources and the specification on how to run them. Containers belonging to the
same Pod are co-located and co-scheduled. Sharing the same context and
communicating each other via \texttt{localhost} or using Inter Process
Communication (IPC). Containers in different Pods will have
different IPs and can communicate via IPC only with a dedicated configuration
(that we do not take into account). Pods are designed to deploy one or
more tightly coupled applications, that in a virtual machine or in a
physical environment will run in the same host, providing a high level
abstraction allowing transparent horizontal scaling. Our proposal Pods are
created using Kubernetes
Deployments\footnote{\url{https://kubernetes.io/docs/concepts/workloads/controllers/deployment/}}:
controllers used to deploy and update Pods. Deployments, at the contrary to
ReplicaSets, support \texttt{rolling-update} functionality, meaning
that Pods definition and configuration can be updated on the fly as well as
rolling back to a prior configuration (in case if Deployment state is
not stable).

\begin{lstlisting}[caption={Example of Deployment definition},
                   captionpos=b, language=yaml, label=chap:impl:lst:deployment]
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: kinks
        image: nginx:1.15.4
        ports:
        - containerPort: 80
\end{lstlisting}
In~\ref{chap:impl:lst:deployment} there is an example of Kubernetes Deployment
for a replica of three Pods running Nginx\footnote{\url{https://www.nginx.com/}}
and all of them are reachable on port $80$. All these Pods are labelled with
\texttt{app: nginx}: labels allow to select only a specified subset of Pods,
for instance to be exposed by Services.

\section{How to reach virtual functions}
The first issue to overcome in our implementation was how to make possible to
reach Pods inside Kubernetes without specifying the IP of the machine on the top
of which they are running. In fact, enabling that possibility would allowed us
to create a flexible and scalable solution. Decoupling Pods from physical
machines makes our proposal independent from the underlying infrastructure:
the
addition or the removal of hosts would be transparent for the code developed,
demanding to Kubernetes the task of manage changes in computational and
storage resources. Kubernetes
services\footnote{\url{https://kubernetes.io/docs/concepts/services-networking/service/}}
allowed us to overcome the problem.

\subsection{Kubernetes Services} \label{chap:impl:subsec:services}
A Kubernetes service is an abstraction that defines a set of Pods and policies
to access them. Pods referenced by a certain Service are usually selected with a
\emph{Label Selector}. In general these labels, added to Pod specifications,
does not provide uniqueness but identify a set of objects.
In the snippet~\ref{chap:impl:lst:srv} there is an example of Service
definition. The service created is called \texttt{my-service} and targets
TCP port 9376 on any Pod labelled with the selector \texttt{app=MyApp}.
Kubernetes will assign to the Service an IP address (also called 
\emph{clusterIP}). \texttt{port} field in the definition is the port on which
the Service can be
reached, instead \texttt{targetPort} is the port on which traffic will be
redirected on Pods. It can be either a valid port number or a string that
identify port name of backend Pods, allowing more flexibility to Pod and Service
creation. Supported protocols are TCP, UDP and SCTP. \texttt{kube-proxy} is
accountable for implementing virtual IP for Services (other than Services with
type \texttt{ExternalName}). 

\begin{lstlisting}[caption={Example of Service definition},
                   captionpos=b, language=yaml, label=chap:impl:lst:srv]
kind: Service
apiVersion: v1
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
\end{lstlisting}

For Service discovery, there were \emph{environment variables} and \emph{DNS}.
To Pods running on a certain Node, \texttt{kubelet} (the daemon that is in
charge to ensure that containers are running and healthy) adds a set of
variables for each Services, that can be used for referencing to it. In the
latter technique, in order to discover services will use the DNS server
cluster
add-on. It watches Kubernetes API to be aware if new Services are created and adds
DNS records for them. In this way all Pods are able to do Service name
resolution automatically. Services can be of type:
\begin{description}
\item[ClusterIP:] to expose services only within the cluster, it is the default
value;
\item[NodePort:] to expose the service on a static port. Outside the cluster is
reachable by requesting \verb!<NodeIP>:<NodePort>!
\item[LoadBalancer:] to expose a service externally, using a cloud provider
load balancer;
\item[ExternalName:] to expose a service with the name expressed in the 
\texttt{externalName}.
\end{description}

All services that we used belong to \texttt{NodePort} category. This choice was
made for a twofold reason. First for the sake of simplicity: binding services to
a predefined port is easy to concatenate services and identify them by port
number.
\texttt{NodePort} type is not suitable for exposing services to the outside
world. However in a production system this functionality can be achieved by
employing an \texttt{ExternalName} service type, which requires the registration
of a domain name.

Taking advantage of the Service concept, we decided that all components of our
solution that
will be deployed on Kubernetes will be exposed by it. In this manner, to
reach a defined component in our solution it is possible to only specify the
name of the service and the port to which it is bound. Kubernetes is in charge
of deciding which Pod to reach (if more replicas of the same function are
available) in the pool labelled to belong to the same service, resolving the
location on which it is deployed.

The SFC implementation underwent several revisions, each time introducing
additional capabilities in the chain provisioning process. In the following we
detail all these iterations pointing out the key features.

\section{First review}
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{firstreview}
  \caption{First review schema}
  \label{chap:impl:img:firstreview}
\end{figure}
This first iteration was an exercise which allowed us to get acquainted with the
Kubernetes platform. At this stage of the project there is no any sort of
automatic chain deployment feature. Moreover
the solution does not contemplate a VNF repository or the associate mechanism
used to retrieve the chains or functions available. Also considering the
objective of this, the data
path the traffic must traverse was statically configured at the chain edge:
traffic inspection is not implemented and the classification is only simulated
returning always the identifier of the chain deployed. This first solution
schema is described in Figure~ \ref{chap:impl:img:firstreview}. Two components
were identified: 
\begin{itemize} 
  \item \enchainer
  \item \vnfs
\end{itemize}
The \enchainer{} component represents the first link on each chain. The
main function of this element is to receive packets from the sender and, in a
more evolved
solution,
to query the classifier retrieving the SFC identifier. 
When a message is received the component is responsible of attaching the chain
specification, that is the list of VNFs comprising it. Below we provide an
example of this encapsulation:
\begin{lstlisting}[language=json]
{
    "message" : "<the original message>",
    "chain": ["<IP vnf1>", "<IP vnf2>", ..., "destination"]
}
\end{lstlisting}
The \texttt{message} field is the original message sent to the chain,
instead the \texttt{chain} field is an array of IPs or Kubernetes service names
that allows to reach the VNFs. The array also specifies the order of VNFs. The
last element
must be the final destination of the packet. 
The \vnf{} in this review are seen as two modules application. One module is
the ``real'' VNF, acts upon the data traffic, while the other manages
the communication in the chain, working as SFF. Each VNF
provides packet forwarding capabilities following this schema:
\begin{enumerate}
  \item once a packet arrives the field \texttt{message} must be read and the
  function represented by the VNF applied;
  \item the field \texttt{chain} must be read and based its values there are two
  possibilities:
  \begin{enumerate}
    \item if array length is greater then $1$ the JSON that wraps the message
    must be reconstructed because the actual \vnf{} is not the last
    element of the chain. \texttt{message} field must be set to the result of
    of the function application and the \texttt{chain} field must be updated
    removing the first element of the array;
    \item if array length is equal to $1$, after the application of the
    transformation of the VNF to the value of the \texttt{message} field,
    message must be delivered to destination, without the JSON wrapping;
    \item otherwise the message must be discarded.
  \end{enumerate}
\end{enumerate}

\subsection{Implementation}
This first implementation was developed in Java\footnote{And available on
this repository: \\\url{https://github.com/Augugrumi/chaining-functionalities}}.
The \enchainer{} component is simply a server implemented using Netty
library\footnote{\url{https://netty.io/}}. The choice of this tool is due
the possibility to serve multiple protocols and for its quick and ease of use.

The \vnf{} component is defined as follows: an interface defines the methods
that must be implemented, an abstract class extends the previous interface
implementing the communication layer of the interface as well as it manages the
JSON wrapping/unwrapping. \vnf{} communication takes advantage of
Rapidoid\footnote{\url{https://www.rapidoid.org/}} library for communication,
both for simplicity and because it gives the opportunity of
managing a large pool of connection in a rapid way. The \vnf{} is an extendible
software artifact and one could change its functional behavior by implementing
the appropriate method. 

\subsection{Kubernetes Deployment}
Two alternative approaches where implemented to deploy service chains exploiting
this
solution: using a single Pod or multiple Pods. The first solution, although
easier
to deploy is not scalable: all the components of the SFC will run on the same
Kubernetes Node while the second approach, instead, makes use of
different machine allowing more degrees of freedom; one could replicate the same
service on different physical machines.

\subsubsection*{Single Pod}
To deploy this solution, the SFC composition is
defined using a single Kubernetes Deployment (in which all the \vnf{} and
the \enchainer{} are specified) and a single Service that exposes the
\enchainer{} and makes it reachable outside the Pod using the Service name.

\subsubsection*{Multiple Pods}
This solution follows a different approach. The \enchainer{} and the \vnfs{} can
be defined with specific Deployment and Service specification.

Even though it result in a more complicate chain definition, it helps to create
a more scalable and flexible solution. In fact, exposing each \vnf{} with a
Service allows the single \vnf{} to be part of different SFCs and provides the
bases for dynamic chain creation and management.. Moreover, handling separately
the components makes it possible to scale in and out the number of single
\vnfs{} or of the \enchainer{} basing on traffic load.

\subsection{Problems}
This first attempt has different issues. First, the \enchainer{} has no
possibility to retrieve the whole original message of a sender: it requires at
the very least the transport layer headers can be read. However it is not
possible due to i) the server is created with Netty that works at application
layers and ii) Java does not allow to create socket at a layer enough low to
allow transport layer header reading. To mitigate this problem and deploy the
solution the adopted workaround was to add to the payload of the message the
missing information. Lower layer header allows the classifier (that is not
implemented yet) to know more information on the communication, so to perform a
better analysis on the treatment to apply on the incoming packets. Another
problem was the JSON wrapping: it does not follow the standard encapsulation
defined for this technology. JSON gave us an ease of use in terms of reading,
writing and encapsulation of packets, however we discovered that an SFC
compliant
encapsulation does not contemplate JSON. Furthermore, even
the communication among VNF works at the application level using POST requests
to send data and lower layer communication can enhance performance. Finally,
in this implementation, the data is always routed through the \enchainer{},
introducing potential bottleneck in terms of processing and communication
latency.

\section{Second review}
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{secondreview}
  \caption{Second review schema}
  \label{chap:impl:img:secondreview}
\end{figure}
This approach is an alternate solution in which components
organization is changed, as depicted in Figure~\ref{chap:impl:img:secondreview}.
The main difference is that forwarding functionality is decoupled from the 
\vnfs{} and provisioned as a distributed separated functional component.

In parallel of the development of this proposal we started the development of
the instrumentation that automatizes the chain deployment.

In this second proposal three components are defined:
\begin{itemize}
  \item \enchainer{}
  \item \dispatcher{}
  \item \vnfs{}
\end{itemize}
\enchainer{} is much the same of the first implementation. Even in this case
the this component must be the first element to be traversed,
wrapping the message into a JSON format (as in the previous solution),
defining the
list of function that traffic must traverse. \vnfs{}, instead, were revisited.
During the development of this second implementation we thought that they must
be decoupled from the mechanism on encapsulation/decapsulation and communicate
with other elements of the platform. The
new element introduced was the \dispatcher{}. It is a centralized component
whose aim is to manage the chain composition and works as follows: it receives
the JSON wrapped message from the \enchainer{} and dissects this in 
\texttt{message} and \texttt{chain} parts. After, it sends the former
value to the first element of the array described from the \texttt{chain} field.
Following this idea, \vnfs{} does not need to be aware of the JSON format.
Basically, the difference between the two approaches is the \dispatcher{}
component, responsible for traffic forwarding.
\subsection{Implementation}
Even this second implementation was developed in Java\footnote{And
available on this repository: \\
\url{https://github.com/Augugrumi/alternative-chaining-functionalities}}. The
\enchainer{} implementation was not changed. To the \vnf{} component
was removed the JSON encapsulation code: as in the previous review a hierarchy
is provided, but this time is only need for a more rapid function development.
Even this prototype allows to create a \vnf{} using different programming languages, only requirements are:
\begin{itemize}
  \item to be able to receive POST requests;
  \item to be able to modify the payload of a POST request, applying the
  function for which it was created;
  \item to be able to reply to the previous request, sending the modified data.
\end{itemize}
The code managing message encapsulation is moved to the \dispatcher{}, making
use of
Rapidoid library for ingoing and outgoing communications.

\subsection{Kubernetes Deployment}
Even for this implementation we provide two different deployments, one that
deploys all the system in a single Pod and the other that exploits the
possibility using multiple Pods. The deployments works as before. In the Single
Pod one the SFC, the \enchainer{} and the \dispatcher{} are specified onto the
same Deployments and one Service is specified to be able to reach the
\enchainer{}. In the Multiple Pods deployment, as previously, each component is
defined by a different Deployment and exposed by a Service.

\subsection{Problems}
This second review, since it is based on the code of the previous one, does not
solve all the problems explained before, as in those regarding application
level communications, the usage of Java as programming language and JSON
wrapping issues. This solution was developed to study a different
approach, using a centralized element, the \dispatcher{}, that can be the
traffic orchestrator and can redirect traffic to the \vnfs{}. 



Also, improving
its capabilities, it could have more information on transformation that packets
must pass through and eventually, reclassify the traffic querying the
classifier. Moreover it provides a complete decoupling between SFC platform and
network functions that must be applied. Thus, there is the drawback of the
centralized approach. This limits the overall scalability since all function
must talk to the same component. Exploiting Kubernetes functionalities it is
possible to augment replicas of the \dispatcher{}, even automatically.
Nevertheless, this approach seems to be not suitable for large traffic loads
and it could be the bottleneck of the platform.

\section{Final proposal}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{finalreview}
  \caption{Final review schema}
  \label{chap:impl:img:finalreview}
\end{figure}

After the second review, we performed a complete change in terms of
implementation and technologies used. We redesigned the possible solution based
on the problems that we found on the first and the second reviews. The first two
approaches implemented unidirectional chains, that is traffic could flow only on
one side, this implementation
allows \emph{bidirectional} traffic instead. 
The final and improved proposal is composed mainly by four components:
\begin{itemize}
  \item \astaire{} implementing the SFF functionality;
  \item \harbor{} implementing the MANO;
  \item \ironhide{} providing the endpoints of the chains;
  \item \roulette{} storing context information.
\end{itemize}
All together they provide the possibility to both create a chain of VNF and
handle it, allowing the automatic deployment. In the following we discuss all
the components in more details.

\subsection{Classification}
One of the key feature of the SFC platform is the capability to classify
traffic based on its type. In fact, this gives the system the capability to
redirect traffic load on the more appropriate chain of network function and, if
needed, dynamically reclassify traffic. Nevertheless, even in the
final implementation we do not take into account the development of a fully
featured classifier because it is out of the scope of this project, although the
latter element is considered in the overall system functioning but is
implemented only as a mock component. Depending on the protocol type used on
transport layer protocol used to transmit traffic to the SFC edge, it gives back
a chain identifier. The protocol supported for now are TCP and UDP.

\subsection{Overlay routing}
When packets are sent from a sender $S$, to a receiver $R$, in this scenario,
they start travelling to the network and reach the our system, where traffic can
be classified and redirect to the appropriate SFC to be handled. During this
elaboration, it is important to maintain transparency (making the sender unaware
of what is happening in the middle of the transmission) and to keep all the
information of the original packet (headers included) for better classification.
To achieve this two main possibilities can be used TUN/TAP tunneling or
encapsulation packets such as IP over IP.

\subsubsection*{TUN/TAP}
\begin{figure}
  \centering \includegraphics[scale=0.5]{tuninterface}
  \caption{TUN interface representation}
  \label{chap:prjan:img:tun}
\end{figure}
TUN and TAP are virtual kernel interfaces that do not have any physical
component, contrary to usual network interfaces. TUN (network TUNnel) operates
at level 3, as IP packets do, TAP (network tap) works instead at layer 2,
dealing with Ethernet packets. In user-space programs they emulate the incoming
of data from external sources and inject packets to the operating system
stack. Interfaces can be created as transient (created and destroyed from the
same program) or persistent (created by some utility and programs attach to
them). once created they are seen as hardware interfaces. In network tunnels
created using this interfaces it is possible to exchange data in a secure way
over a public network and to transmit packets trough a network even if a certain
protocol is not supported, thanks to the encapsulation that allows, then,
to maintain unaltered packets inside the tunnel.

\subsubsection*{Packet encapsulation}
\begin{figure}[t]
  \centering \includegraphics[scale=0.5]{IPoverIP}
  \caption[IP Encapsulation within IP packet schema]{A schema of IP
    Encapsulation within IP. Is noticeable how the original packet becomes the
    payload of a new one, thus being encapsulated. All the information
    regarding the original packet remains untouched: when this kind of packet
    gets send from a machine to another, the OS will remove the new packet
    header and will present to the user-space program receiving the data its
    payload: a IP packet. Packets encapsulation can be applied with higher
    layers too, thus allowing for TCP and UDP packet encapsulation.}
  \label{chap:prjan:img:ip_over_ip}
\end{figure}
IP over IP encapsulation allows us to preserve the original packet in order for
it
to become the payload of a new packet, that can be elaborated and modified
accordingly, until it gets decapsulated and eventually sent to the final
receiver. When it arrives, the receiver is not aware of the packet
elaboration, since all the original headers were preserved (or slightly
modified). In particular, IP over IP allow the packets to pass through
intermediate destinations that otherwise would not be selected. Split-TCP,
instead, ``divides'' the TCP session into two parts using proxies: each proxy
pretends to be the opposite endpoint in each direction. It creates a
multi-overlay-hop path where for each hop there could be an independent TCP
connection and algorithms. In order to achieve that, a session
table needs to be established and continuously updated, while TCP headers need
to be accordingly modified to allow data transmission through the proxies and
endpoints. Nonetheless, care should be given to packets path: in order to avoid
ill-calculated congestion windows, the incoming packets need to follow the same
path for the same instantiated connection (or an equivalent one). This
phenomenon is based on the intermediate proxies that create the route: it is the
case, for example, of when an intermediate node sends back a spoofed ACK before
the original packet is delivered to the real consignee. In this scenario, the
sender, unaware of the proxy, will miscalculate the sending window based on the
too low RTT. Reliability and security are two other problems of TCP splitting,
especially because the end-to-end TCP logic is no longer maintained: a server
failure may cause an unaware client to believe that all the packets have
reached the destination. TCP session recreation seems burdensome and it does not
seem to add any significant benefit at first sight, but it does offer the
flexibility to perform extensive packet manipulation and use different TCP
flavors, hence increasing performance in heterogeneous networks.

\vspace*{1cm}

\noindent In the final review we tried both the approaches. The tunnelling
approach is not suitable for communication among different links of the chain:
in fact it will create a strong relation among the Pods, nullifying the use of
Kubernetes services. Moreover, Pods are mortal and if a Pod dies in the tunnel,
some time is required to reconstruct it. Ultimately, creating a tunnel makes it
more difficult to update a chain or reclassify a packet during the SFC traverse
because different tunnels must be created. Also, using a TUN tunnel the
communication between the external world and the SFC would be more cumbersome.
On the other end it will make it possible to easily keep all the headers of the
original packets easily and make the communication more secure. Encapsulating
packets give us a more flexible approach and we decided to utilize it even if it
does not allow us to keep header information in a simple manner. Both for TCP
and for UDP external connection, the communication among links in the chain uses
UDP protocol. We choose UDP-encapsulation because this protocol removes some of
the TCP overhead: it does not implement any form of congestion avoidance
mechanisms and it does not create a connection among sender and receiver so
generally it has a lower latency. On the other hand it is unreliable and
congestion scenarios are not managed.

\subsection{Ingresses and egresses: endpoints of the chain}
\label{chap:impl:subsec:endpoints}
In both previous implementations there was a component that handles the
traffic before it enters the SFC domain. In this final solution we developed 
\ironhide{}\footnote{And available on this repository: 
\url{https://github.com/Augugrumi/ironhide}}. This component is useful not only
for managing incoming packets but even the communication with the final
destination. We called the element of the solution that is accountable for the
treatment of incoming packets \emph{ingress} and similarly the component that
handle the communication with the destination is called \emph{egress}. Together
they are chains \emph{endpoints}. \ironhide{} is developed in C++ and it
provides all the functionalities used to handle incoming packets from the outside
of
the chain and delivering them after the SFC optimization. When \ironhide{} works
as \ingress{} it basically operates as a server that accepts both TCP and UDP
packets. Once a packet arrives it is forwarded to the classifier, along with the
TCP/UDP-IP headers, which chooses the SFC to which it must be forwarded.

The result of the classification is used in the creation of the NSH. During the
implementation of our solution we only considered the SFC encapsulation with a
Fixed-Length Context Header: we reduce as possible the usage of metadata to be
exchanged within the header to be as independent as possible to the header
typology, and the semantic is represented in Figure~\ref{chap:impl:img:nsh}.

\begin{figure}
  \centering \includegraphics[width=\textwidth]{nshsol}
  \caption{NSH Fixed-Length Context Header field usage}
  \label{chap:impl:img:nsh}
\end{figure}

The \texttt{Base Header} part of the header is used as defined in the RFC 8300.
The \texttt{Service Path Header} is slightly. In fact the 
\texttt{Service Path Identifier} is used as described into the standard but the
usage of \texttt{Service Index} differ. The RFC defines that it must
start with all bits set to 1 an each time the packet pass through a link the
field value have to be decremented. We have done the opposite just for the sake
of clarity and simplicity. The \texttt{Context Header} is comprised of:
\begin{description}
  \item[Source IP:] 32-bit representation of IPv4 original source address;
  \item[Destination IP:] 32-bit representation of IPv4 original destination
  address;
  \item[Source Port:] 16-bit representation of port used by the packet source;
  \item[Destination Port:] 16-bit representation of port used by the destination
  to receive packet;
  \item[Direction (D):] 1-bit chain traversal direction representation.
\end{description}
The first four fields are necessary to make the solution carry information on of
the
packet source and destination throughout the chain. IPv4
representation is used, but improvements and usage of variable length context
header can enhance the solution allowing both IPv4 and IPv6 address
representation. The \texttt{Direction} bit instead is used to make components of
the chain aware of the direction on which the SFC is traversed. This field is
not strictly necessary but it allows to retrieve this information without
actually checking which is the previous link that has managed a given packet. 

Finally, before delivering the packet to the first element of the chain the
\ingress{} records an entry on a local map to remember the connection with
the sender of the message. This is of paramount importance in bidirectional
communications: the connection with the sender is kept alive to
subsequently deliver the answer of the receiver. In fact, if the latter sends
back data to the sender, the \ingress{} is able to deliver it thanks this
saved information.

\texttt{Egress} component waits for communication from the chain an it is in
charge of forwarding messages to the destination initially specified from the
sender before the manipulation of the SFC. To accomplish this task it is
necessary to establish a connection, TCP or UDP depending on the original
sender communication protocol used, with the final receiver. In a mirrored way
as the \ingress{}, the \egress{} after establishing the connection with the
destination, record communication information into a map. This in useful when
more than a single packet must be exchange by end-to-end element of the
connection.

In sequence diagrams shown in Figure~\ref{chap:impl:img:ingrdiagram}
and Figure~\ref{chap:impl:img:egrdiagram} are shown the steps.

\subsection{Roulette: Context Information Policies and Chain Definition
provider}
In bidirectional chains, involving a continuous interaction between a
sender and a receiver, we need a functionality keeping the
connections 
between sender/receiver and \ingress{}/\egress{} alive. Also we need the
capability to 
retrieve the \ingress{}/\egress{} in case of a set of replicas of these
components are deployed.
\begin{exmp}
  Suppose we have a set $I=\{I0, I1, I2\}$ of \ingresses{} and a set $E=\{E0,
  E1\}$ of \egresses{} and suppose that from the sender $S$ packets reach the
  destination $R$ following the path:
  \begin{equation*}
    \texttt{S} \Longrightarrow \texttt{I0} \Longrightarrow \texttt{sfc}
    \Longrightarrow \texttt{E1} \Longrightarrow \texttt{R}
  \end{equation*}
  If the response does not reach again the \ingress{} $I0$ but, for instance,
  $I2$, the connection between $S$ and $I0$ must be aborted and a new connection
  among $S$ and $I2$ must be established. The same is whether different
  \egresses{} are used. This semantics can require modification both on sender
  and on receiver, making they to be able to support these disconnections. At
  the contrary of using always the same \ingress{} and \egress{} for a given
  connection.
\end{exmp}

\noindent
To accomplish this goal we developed \roulette{}\footnote{And available on
this repository: \url{https://github.com/Augugrumi/roulette}}. This component
orchestrates SFC ingresses and egresses in this way: once a new connection is
opened on a certain \texttt{ingress}, the latter has the task to save on the
\roulette{} database an entry specifying its information (IP) for the
communication and on the SFC used. Once the packet has traversed the whole chain
and it reaches an \texttt{egress}, the same entry is updated, adding even the 
\texttt{egress} information. Thanks to this component it is possible to bind a
communication to a specified pair \verb!<ingress,egress>!.

\roulette{} was developed in Java using
Spark\footnote{\url{http://sparkjava.com/}}, used to manage requests and
connected to a MongoDB\footnote{\url{https://www.mongodb.com/}} database. We
choose MongoDB as database because it allows to have a flexible data schema,
useful in particular in a test environment.

The duty of this component is not only to manage \ingresses{} and \egresses{}
information but it keeps also the definitions of the chains. In fact, once a
classifier returns the identifier of the SFC that traffic must traverse, the
complete definition or the link of the chain in a certain position can be
retrieved querying this element of the platform. SFC definition follows this
pattern:

\begin{lstlisting}[caption={Definition of an SFC on \roulette{}.}, captionpos=b,
                   language=json]
{
  "si" : [
    {
      "url" : <vnf0-service-name>,
      "port": <vnf0-port>
    },
    ...,
    {
      "url" : <vnfn-service-name>,
      "port": <vnfn-port>
    }
  ]
}
\end{lstlisting}

\noindent
where the \texttt{si} field is an array of objects that identifies a single VNF
that belongs to the chain. It is composed by the field \texttt{url} in which the
identifier of the Service that allows to reach a certain VNF is specified and
the field \texttt{port} specifies the port that must be used to contact the
Service.

We chose to use the same component for both SFC definitions and 
\ingress{}/\egresses{} pairing only for time constraint. There are no other
limitations that impose that these two pieces of information must be handled by
the same component. 

As the chains, \roulette{} is deployed on Kubernetes using a Service to expose
its functionalities. The usage of Kubernetes allows even to have a database
replica to avoid data loss and to provide high availability of information.

\subsection{Proxies}
\texttt{Ingress} is implemented as a server that allows both TCP and UDP
connection on a given port. In order to do so they use \verb!SOCK_STREAM! and
\verb!SOCK_DGRAM! sockets respectively. This approach has a drawback: opening
this kind of sockets does not allow to read even all TCP/UDP headers information
of the
incoming packets. In a mirrored way, \egresses{} opens same type of sockets to
communicate with the destination, so neither in this case it is possible to read
response headers. To preserve them we implemented a workaround in form of a
Python script that recreates original header that must be put between the
sender/receiver and the \ingress{}/\egresses{}, that works as a proxy that add
to the payload of the original message even the header recreated. This script
was developed both for UDP and TCP protocols.

The solution can be enhanced removing the proxies exploiting
\texttt{raw sockets}. Redesigning \ironhide{} to make it able to manage level 2
communication (Ethernet packets) will make that every packet received will
be read along with its headers. This approach has a main disadvantage: in TCP
connections part of the TCP logic must be re-implemented, as TCP-IP header
creation or ACK handling. For this reason we does not implemented it. UDP,
instead, does not requires too much modification and communication between 
\egress{} and the destination was developed using level 2 sockets: this even
shows that proxies are not necessary components of the solution proposed but
they are only needed due to our implementation.

Proxies are even used to redirect traffic directed to the a certain destination
to the chain: this work has to be done to the endpoints of the chain to
completely remove these components.

\subsection{Astaire: a Service Function Forwarder implementation}
The component that enables the communication with VNF and the creation of chains
is \astaire{}\footnote{And available on this repository:
\url{https://github.com/Augugrumi/Astaire}}. This element, developed in C++,
works as the SFF in the standard architecture overview. As mentioned before,
communication among \ingresses{}/\egresses{} and VNFs and among virtual
functions is implemented using UDP protocol. This choice was made
due to the fact that UDP is lightweight with respect to TCP. 

\astaire{} main tasks are to receive and forward packets and it is implemented
as an SFF that manages a unique VNF. Since we suppose that all the VNFs that our
system handles are \emph{SFC unaware} we incorporated the NSH handling and SFC
proxy functionality in this element. To deal with \emph{SFC aware} VNFs the only
modification required is to not remove this encapsulation. Basically once a
packet is received \astaire{} removes the NSH encapsulation and sends it to the
associated
virtual function. In this implementation we suppose that \astaire{} is able to
reach a VNF as a
simple method call or invoking a C++ or Java program whose path is passed to 
\astaire{} as argument during launch. The VNF must be able to manage an array of
bytes that represent the packet that is traversing the chain, to which is
removed the NSH (so headers and payload of the original packet, eventually
modified from previous virtual functions). Call to VNF is totally decoupled
to forwarding mechanism, so it could be expanded allowing the possibility to
call a function in terms of sending a packet and waiting for the response. When
the VNF has finished to treat the packet, based on the NSH removed before,
calculate the next step to which traffic must be forwarded and the new header.

To retrieve the next element \astaire{} reads the \texttt{Service Path
Identifier} and the \texttt{Service Index} fields of the \texttt{Service Path
Header}. After it checks in a local map if this values was used yet: in this
case in the map are saved information (address, port) of the next element.
Whether this information is not present it have to query \roulette{} to recover
this data. The \texttt{Service Index} field is incremented before this check. In
case the instance of \astaire{} is the last link of the SFC \roulette{} will
return:
\begin{itemize}
  \item the name of the \egress{} service in case that the current packet is the
  first packet that traverse the chain exchanged between a certain pair
  \verb!<sender,receiver>!; or
  \item data of the \egress{} used for the current communication.
\end{itemize}
Local maps update its information periodically in an automated way: this makes
it
possible to be responsive to SFC updates or changes on Pods configuration. The
update period is set to 2 minutes by default but further enhancement must set it
more accurately. Since Pods can go down for their nature a system to detect
Pods faults and update local maps can make the overall platform less prone to
packet loss.

\astaire{} modifies the NSH with the field
\texttt{Time To Live} decremented by one and the field \texttt{Service Path
Identifier} increased by one. This mechanism also prevent the creation of
cycles: either the \texttt{TTL} field goes to zero
and the packet will be discarded or the \texttt{SPI} will no more point to an
element of the chain and traffic will be redirected to an \egress{}.



\subsection{Harbor: Chain Manager and Orchestrator}
To handle chains and virtual functions we developed
\harbor{}\footnote{\url{https://github.com/Augugrumi/harbor/}}. This component
works as the MANO of our platform. We developed \harbor{} in a manner that it
is not possible to deploy single VNFs. This because the single VNF does not
allow the management of incoming and outgoing traffic without \ingresses{} and
\egresses{}. This component is only a prototype of a fully featured orchestrator
even regarding VIM handling. In fact, due to time constraint the communication
with Kubernetes APIs is implemented just at a basic level, allowing a limited
number of operations. It is implemented in Java using Spark to expose a REST API
with which is possible to handle SFCs. Despite other components of this
implementation \harbor{} is not deployed on Kubernetes.

\harbor{} handles both VNF and SFC catalogs: it is possible to get the list of
saved definitions, to add and update an existing one and to remove one of them.
Moreover, since it is an orchestrator it allows to launch a new SFC instance
from a previous saved definition and to stop a running chain instance.

An example of VNFs and SFC that can be deployed using \harbor{} and our platform
is showed in~\ref{chap:impl:lst:vnfexample}. As explained 
in~\ref{chap:impl:subsec:services} all the function that will run on Kubernetes
are exposed using a Service, decoupling the use of a certain function to the way
it can be called.
\begin{lstlisting}[caption={Example of VNF definition}, captionpos=b,
                   language=yaml, label=chap:impl:lst:vnfexample]
kind: Deployment
apiVersion: extensions/v1beta1
metadata:
 name: astaire-deployment
 namespace: default
 labels:
   k8s-app: astaire-vnf
spec:
 replicas: 1
 selector:
   matchLabels:
     k8s-app: astaire-vnf
 template:
   metadata:
     labels:
       k8s-app: astaire-vnf
       name: astaire-vnf
   spec:
     containers:
       - name: astaire
         image: augugrumi/astaire:latest
         args: ["-u", "-r", "roulette-service:80"]
         imagePullPolicy: Always
         ports:
           - name: udp
             containerPort: 8767
             protocol: UDP
---
kind: Service
apiVersion: v1
metadata:
 name: astaire-service
 namespace: default
spec:
 selector:
   k8s-app: astaire-vnf
 ports:
   - name: udp
     port: 8767
     protocol: UDP
 type: NodePort
\end{lstlisting}

In~\ref{chap:impl:lst:sfcexample} is shown an example of SFC definition. It is
implemented using only as an array of VNFs definition. The array gives even the
order of the functions in the chain.
\begin{lstlisting}[caption={Example of SFC definition}, captionpos=b,
                   language=json, label=chap:impl:lst:sfcexample]
{
    "ns" : [
                {
                    "id": "astaire-service"
                },
                ...
            ]
}
\end{lstlisting}

Once a definition of a chain is updated, this transformation is reflected to
running instances of the SFC (if any) to make the system more flexible.
Monitoring functionalities implemented are really limited: \harbor{} can only
provide information if a certain SFC is running and if components of that SFC
instance are running. At this stage it is not developed to provide data on Pods
faults or misbehavior. Furthermore, neither scalability features are implemented
yet.

Finally, this architectural component has the task to set the description of
SFCs, in order to make them available to \ingresses{}, \egresses{} and
\astaire{} instances.

\begin{figure}
  \centering \includegraphics[scale=0.35]{ingress_seq_diagram}
  \caption[Sequence diagram - Incoming traffic management]{In the diagram is
  shown how the system works when a packet arrives from a new end-user. 
  \texttt{Ingress} manages packet classification and encapsulation and forwards
  it to the first element of the chain, after the registration on \roulette{}. 
  \astaire{} goal instead is to manage communication and to provide the packet
  to the SFC unaware function \texttt{VNF0}.}
  \label{chap:impl:img:ingrdiagram}
\end{figure}

\begin{figure}
  \centering \includegraphics[scale=0.35]{egress_seq_diagram}
  \caption[Sequence diagram - Outgoing traffic management]{In the diagram is
  shown how the system works when a packet has to be delivered to destination.
  \texttt{Egress} registers itself on \roulette{}. After it decapsulates the
  packet and delivers it to destination. The (eventual) response of the
  destination is classified, encapsulated and sent to the right chain.}
  \label{chap:impl:img:egrdiagram}
\end{figure}


\section{Implementation problems} \label{chap:impl:sec:problems}
This final approach has still some open issues that have to be addressed in the
future. \harbor{} is proof of concept
implementation: as stated before a fully functional MANO must be able to provide
more information on running SFCs and VNFs. In addition, the possibility to
manual scaling elements of a running chain can improve even more this component.
Definition of the VNFs allows to define the ReplicaSet (since they are YAML
file used by Kubernetes) but a future release of \harbor{} must provide
the possibility to define rules to autoscale the components. Kubernetes
provide this possibility taking advantage of \texttt{autoscaling/v2beta2} API
version\footnote{\sloppy\url{https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough}},
in which the set of Pods can be increased/decreased basing on specified metrics.

\roulette{}, that has to keep track of chain endpoints (the pair 
\verb!<ingress,egress>!) of a communication and the SFCs definitions, is
a centralized element of the platform, as it is used and implemented in this
final review. It was thought as element on which the blocks of the final
proposal have to most frequently perform reads than writes, but increasing the
number of connection that exploits the chain this implementation can be not
enough and a distributed and more scalable approach have to be used. An
alternative solution is to divide it into two different element: one that gives
to endpoints and to \astaire{} instances information on the chain and one that
take care of pairing ingresses and egresses. The former components will be
written only by the MANO. The latter piece of information can, instead, be added
to a variable length NSH. Exploiting this possibility the platform will have a
lighter backend and the pair \verb!<ingress, egresses>! will be added
directly from this components to the header and forwarded by \astaire{}.

\texttt{Ingresses} and \egresses{} implementation problems lay on the socket
handling. Low level interface must be used in a future implementation, making
endpoints able to read headers and to be \emph{performance enhancement proxies}.
Another improvement is to better support end-to-end communication between sender
and receiver. If the sender (or the receiver) disconnects from the corresponding
endpoint it is transparent to the counterpart. The same happens in case of
packet loss. Mechanism to manage these scenarios must be develop (as, in
general, when proxies are used).

The communication inside the platform takes advantage of the lightweight UDP
protocol approach but does not implement internal mechanisms of packet recovery.
It could be useful to develop some sort of local retransmission in case of
packet loss to avoid to give this responsibility to the external sources.

Finally, \astaire{} is implemented as an SFF that take care of only one VNF. It
decreases complexity of the component but it implies that one \astaire{}
instance must run for each VNF. Moreover our solution is only a simple
implementation and we do not tested it under a large traffic load, where right
connection handling has a critical role.
